{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95467,"databundleVersionId":11364276,"sourceType":"competition"},{"sourceId":11210713,"sourceType":"datasetVersion","datasetId":7000242}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" pip install pymupdf pillow pytesseract camelot-py[cv] tabula-py nltk sentence-transformers transformers chromadb bert-score tenacity pydantic-settings scikit-learn torch torchvision tqdm requests openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:05:55.316752Z","iopub.execute_input":"2025-05-03T04:05:55.317191Z","iopub.status.idle":"2025-05-03T04:06:29.518651Z","shell.execute_reply.started":"2025-05-03T04:05:55.317153Z","shell.execute_reply":"2025-05-03T04:06:29.517793Z"}},"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\nCollecting tabula-py\n  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting chromadb\n  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (9.0.0)\nCollecting pydantic-settings\n  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\nCollecting camelot-py[cv]\n  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (8.1.7)\nRequirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (5.2.0)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (1.26.4)\nRequirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (3.1.5)\nCollecting pdfminer-six>=20240706 (from camelot-py[cv])\n  Downloading pdfminer_six-20250416-py3-none-any.whl.metadata (4.1 kB)\nCollecting pypdf<4.0,>=3.17 (from camelot-py[cv])\n  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (2.2.3)\nRequirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (4.12.2)\nRequirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv]) (4.10.0.84)\nCollecting pypdfium2>=4 (from camelot-py[cv])\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: distro in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.9.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.11.0a2)\nCollecting chroma-hnswlib==0.7.6 (from chromadb)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nCollecting fastapi==0.115.9 (from chromadb)\n  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.21.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.13.0)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.23.0)\nCollecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.5)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic-settings)\n  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->camelot-py[cv]) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\nRequirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\nCollecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (44.0.1)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.29.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->camelot-py[cv]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->camelot-py[cv]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->camelot-py[cv]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->camelot-py[cv]) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->camelot-py[cv]) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.21.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\nDownloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\nDownloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\nDownloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\nDownloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20250416-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\nDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading camelot_py-1.0.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=54c103b7c020c7ef3579f578b445c1c266b978a98baec604fabd85ea224e9696\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, uvicorn, typing-inspection, python-dotenv, pyproject_hooks, pypdfium2, pypdf, pymupdf, protobuf, opentelemetry-util-http, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, pydantic-settings, pdfminer-six, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, onnxruntime, chroma-hnswlib, camelot-py, tabula-py, chromadb, bert-score\n  Attempting uninstall: pypdf\n    Found existing installation: pypdf 5.3.0\n    Uninstalling pypdf-5.3.0:\n      Successfully uninstalled pypdf-5.3.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.29.0\n    Uninstalling opentelemetry-api-1.29.0:\n      Successfully uninstalled opentelemetry-api-1.29.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.50b0\n    Uninstalling opentelemetry-semantic-conventions-0.50b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.50b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.29.0\n    Uninstalling opentelemetry-sdk-1.29.0:\n      Successfully uninstalled opentelemetry-sdk-1.29.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 bert-score-0.3.13 build-1.2.2.post1 camelot-py-1.0.0 chroma-hnswlib-0.7.6 chromadb-1.0.7 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 onnxruntime-1.21.1 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 pdfminer-six-20250416 posthog-4.0.1 protobuf-5.29.4 pydantic-settings-2.9.1 pymupdf-1.25.5 pypdf-3.17.4 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 tabula-py-2.10.0 typing-inspection-0.4.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:06:29.519808Z","iopub.execute_input":"2025-05-03T04:06:29.520086Z","iopub.status.idle":"2025-05-03T04:06:33.625903Z","shell.execute_reply.started":"2025-05-03T04:06:29.520060Z","shell.execute_reply":"2025-05-03T04:06:33.625102Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\nRequirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.1)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20250327->pdfplumber) (44.0.1)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\nDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six, pdfplumber\n  Attempting uninstall: pdfminer.six\n    Found existing installation: pdfminer.six 20250416\n    Uninstalling pdfminer.six-20250416:\n      Successfully uninstalled pdfminer.six-20250416\nSuccessfully installed pdfminer.six-20250327 pdfplumber-0.11.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install --upgrade faiss-cpu ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:06:33.627224Z","iopub.execute_input":"2025-05-03T04:06:33.627543Z","iopub.status.idle":"2025-05-03T04:06:38.629594Z","shell.execute_reply.started":"2025-05-03T04:06:33.627518Z","shell.execute_reply":"2025-05-03T04:06:38.628731Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:33:24.626129Z","iopub.execute_input":"2025-05-03T04:33:24.626477Z","iopub.status.idle":"2025-05-03T04:33:33.413009Z","shell.execute_reply.started":"2025-05-03T04:33:24.626447Z","shell.execute_reply":"2025-05-03T04:33:33.411958Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain-core<1.0.0,>=0.3.56 (from langchain-community)\n  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.24 (from langchain-community)\n  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.9.1)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.24->langchain-community)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.0a2)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.29.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.2.2)\nDownloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.58-py3-none-any.whl (437 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nInstalling collected packages: httpx-sse, async-timeout, langchain-core, langchain-text-splitters, langchain, langchain-community\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 httpx-sse-0.4.0 langchain-0.3.25 langchain-community-0.3.23 langchain-core-0.3.58 langchain-text-splitters-0.3.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport re\nimport fitz  # PyMuPDF\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nimport matplotlib.pyplot as plt\nfrom bert_score import score\nimport logging\nimport json\nfrom typing import List, Dict, Tuple, Any, Optional\nimport requests\nfrom tqdm import tqdm\nimport gc  # For garbage collection\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass MultiModalRAG:\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize the MultiModal RAG system\n        \n        Args:\n            config: Configuration dictionary with model paths and parameters\n        \"\"\"\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n        \n        # Initialize models\n        self._initialize_models()\n        \n        # Initialize vector DB\n        self.chroma_client = chromadb.PersistentClient(path=config[\"chroma_db_path\"])\n        \n        # Create collections or get existing ones\n        self._setup_collections()\n        \n    def _initialize_models(self):\n        \"\"\"Initialize text and image embedding models\"\"\"\n        logger.info(\"Loading text embedding model...\")\n        self.text_embedding_model = SentenceTransformer(\n            self.config[\"text_embedding_model\"], \n            device=self.device\n        )\n        \n        logger.info(\"Loading CLIP model for image embeddings...\")\n        self.clip_model = CLIPModel.from_pretrained(self.config[\"clip_model\"]).to(self.device)\n        self.clip_processor = CLIPProcessor.from_pretrained(self.config[\"clip_model\"])\n        \n        # Text splitter for chunking\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.config[\"chunk_size\"],\n            chunk_overlap=self.config[\"chunk_overlap\"],\n            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n        )\n        \n    def _setup_collections(self):\n        \"\"\"Set up or get existing collections in ChromaDB\"\"\"\n        try:\n            self.text_collection = self.chroma_client.get_collection(\"text_embeddings\")\n            self.image_collection = self.chroma_client.get_collection(\"image_embeddings\")\n            logger.info(\"Retrieved existing collections from ChromaDB\")\n        except:\n            logger.info(\"Creating new collections in ChromaDB\")\n            self.text_collection = self.chroma_client.create_collection(\n                name=\"text_embeddings\",\n                metadata={\"hnsw:space\": \"cosine\"}\n            )\n            self.image_collection = self.chroma_client.create_collection(\n                name=\"image_embeddings\",\n                metadata={\"hnsw:space\": \"cosine\"}\n            )\n\n    def process_pdf(self, pdf_path: str) -> Tuple[List[Dict], Dict[int, Dict]]:\n        \"\"\"\n        Process PDF to extract text chunks and images\n        \n        Args:\n            pdf_path: Path to the PDF file\n            \n        Returns:\n            tuple: (text_chunks, images)\n        \"\"\"\n        logger.info(f\"Processing PDF: {pdf_path}\")\n        if not os.path.exists(pdf_path):\n            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n            \n        doc = fitz.open(pdf_path)\n        \n        # Extract text and create text chunks\n        all_text = \"\"\n        page_texts = {}\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            text = page.get_text()\n            page_texts[page_num] = text\n            all_text += text + \"\\n\\n\"\n            \n        # Split text into overlapping chunks\n        raw_chunks = self.text_splitter.split_text(all_text)\n        \n        # Process chunks to add metadata\n        text_chunks = []\n        for i, chunk in enumerate(raw_chunks):\n            # Find which page this chunk is likely from\n            max_overlap = 0\n            chunk_page = 0\n            \n            for page_num, page_text in page_texts.items():\n                overlap = len(set(chunk.split()).intersection(set(page_text.split())))\n                if overlap > max_overlap:\n                    max_overlap = overlap\n                    chunk_page = page_num\n            \n            text_chunks.append({\n                'id': f\"chunk_{i}\",\n                'content': chunk,\n                'page': chunk_page\n            })\n            \n        # Extract images\n        logger.info(\"Extracting images from PDF...\")\n        images = {}\n        figure_table_pattern = re.compile(r'(Figure|Table)\\s+(\\d+)[:\\.]')\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            page_text = page_texts[page_num]\n            \n            # Find all figure/table references in the text\n            matches = figure_table_pattern.findall(page_text)\n            \n            # Extract images from the page\n            for img_index, img in enumerate(page.get_images(full=True)):\n                xref = img[0]\n                base_image = doc.extract_image(xref)\n                image_bytes = base_image[\"image\"]\n                \n                # Try to associate with a figure/table number\n                for match in matches:\n                    fig_type, fig_num = match\n                    fig_num = int(fig_num)\n                    \n                    if fig_num not in images:\n                        try:\n                            pil_image = Image.open(BytesIO(image_bytes))\n                            \n                            # Filter out very small images that might be icons or decorations\n                            if pil_image.width < 50 or pil_image.height < 50:\n                                continue\n                                \n                            # Store the image\n                            images[fig_num] = {\n                                'id': fig_num,\n                                'data': image_bytes,\n                                'page': page_num,\n                                'type': fig_type,\n                                'pil_image': pil_image,\n                                'caption': f\"{fig_type} {fig_num}\"\n                            }\n                            \n                            # Try to find caption text\n                            caption_pattern = re.compile(f\"{fig_type}\\\\s+{fig_num}[:\\\\.\\s]+(.*?)(?=\\\\n\\\\n|$)\", re.DOTALL)\n                            caption_match = caption_pattern.search(page_text)\n                            if caption_match:\n                                images[fig_num]['caption'] = caption_match.group(1).strip()\n                            \n                            break\n                        except Exception as e:\n                            logger.warning(f\"Error processing image {img_index} on page {page_num}: {e}\")\n        \n        logger.info(f\"Extracted {len(text_chunks)} text chunks and {len(images)} images\")\n        return text_chunks, images\n\n    def generate_embeddings(self, text_chunks: List[Dict], images: Dict[int, Dict]) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate embeddings for text chunks and images\n        \n        Args:\n            text_chunks: List of text chunks with metadata\n            images: Dictionary of images with metadata\n            \n        Returns:\n            tuple: (text_embeddings, image_embeddings)\n        \"\"\"\n        logger.info(\"Generating text embeddings...\")\n        text_embeddings = []\n        \n        # Process in batches to prevent memory issues\n        batch_size = 32\n        for i in range(0, len(text_chunks), batch_size):\n            batch = text_chunks[i:i+batch_size]\n            texts = [chunk['content'] for chunk in batch]\n            \n            # Generate embeddings\n            embeddings = self.text_embedding_model.encode(texts)\n            \n            for j, embedding in enumerate(embeddings):\n                chunk = batch[j]\n                text_embeddings.append({\n                    'id': chunk['id'],\n                    'embedding': embedding,\n                    'content': chunk['content'],\n                    'page': chunk['page']\n                })\n                \n        logger.info(\"Generating image embeddings...\")\n        image_embeddings = []\n        \n        # Process images\n        for img_num, img_data in images.items():\n            try:\n                # Process image with CLIP\n                pil_image = img_data['pil_image']\n                inputs = self.clip_processor(images=pil_image, return_tensors=\"pt\").to(self.device)\n                \n                with torch.no_grad():\n                    image_features = self.clip_model.get_image_features(**inputs)\n                    \n                # Convert to numpy and normalize\n                image_embedding = image_features.cpu().numpy().flatten()\n                image_embedding = image_embedding / np.linalg.norm(image_embedding)\n                \n                # Create text embedding for the caption\n                caption = img_data.get('caption', f\"Figure/Table {img_num}\")\n                caption_embedding = self.text_embedding_model.encode(caption)\n                \n                # Combine image and caption embeddings (weighted average)\n                combined_embedding = 0.7 * image_embedding + 0.3 * caption_embedding\n                combined_embedding = combined_embedding / np.linalg.norm(combined_embedding)\n                \n                image_embeddings.append({\n                    'id': f\"img_{img_num}\",\n                    'embedding': combined_embedding,\n                    'image_num': img_num,\n                    'page': img_data['page'],\n                    'type': img_data['type'],\n                    'caption': caption\n                })\n            except Exception as e:\n                logger.warning(f\"Error generating embedding for image {img_num}: {e}\")\n                \n        logger.info(f\"Generated {len(text_embeddings)} text embeddings and {len(image_embeddings)} image embeddings\")\n        return text_embeddings, image_embeddings\n\n    def store_in_db(self, text_embeddings: List[Dict], image_embeddings: List[Dict]):\n        \"\"\"\n        Store embeddings in the vector database\n        \n        Args:\n            text_embeddings: List of text embeddings with metadata\n            image_embeddings: List of image embeddings with metadata\n        \"\"\"\n        logger.info(\"Storing text embeddings in vector DB...\")\n        \n        # Store text embeddings\n        if text_embeddings:\n            ids = [str(item['id']) for item in text_embeddings]\n            embeddings = [item['embedding'].tolist() for item in text_embeddings]\n            metadatas = [{\"page\": item['page'], \"content\": item['content']} for item in text_embeddings]\n            documents = [item['content'] for item in text_embeddings]\n            \n            # Add to collection in batches to prevent memory issues\n            batch_size = 100\n            for i in range(0, len(ids), batch_size):\n                self.text_collection.add(\n                    ids=ids[i:i+batch_size],\n                    embeddings=embeddings[i:i+batch_size],\n                    metadatas=metadatas[i:i+batch_size],\n                    documents=documents[i:i+batch_size]\n                )\n        \n        logger.info(\"Storing image embeddings in vector DB...\")\n        # Store image embeddings\n        if image_embeddings:\n            ids = [item['id'] for item in image_embeddings]\n            embeddings = [item['embedding'].tolist() for item in image_embeddings]\n            metadatas = [{\n                \"image_num\": item['image_num'],\n                \"page\": item['page'], \n                \"type\": item['type'],\n                \"caption\": item['caption']\n            } for item in image_embeddings]\n            documents = [item['caption'] for item in image_embeddings]\n            \n            self.image_collection.add(\n                ids=ids,\n                embeddings=embeddings,\n                metadatas=metadatas,\n                documents=documents\n            )\n            \n        logger.info(\"Successfully stored embeddings in vector DB\")\n\n    def retrieve(self, query: str, top_k_text: int = 5, top_k_images: int = 2) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Retrieve relevant text chunks and images for a query\n        \n        Args:\n            query: The query string\n            top_k_text: Number of text chunks to retrieve\n            top_k_images: Number of images to retrieve\n            \n        Returns:\n            tuple: (retrieved_texts, retrieved_images)\n        \"\"\"\n        logger.info(f\"Retrieving information for query: {query}\")\n        \n        # Generate query embedding\n        query_embedding = self.text_embedding_model.encode(query).tolist()\n        \n        # Retrieve from text collection\n        text_results = self.text_collection.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k_text\n        )\n        \n        # Retrieve from image collection\n        image_results = self.image_collection.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k_images\n        )\n        \n        # Format text results\n        retrieved_texts = []\n        for i, doc in enumerate(text_results['documents'][0]):\n            retrieved_texts.append({\n                'content': doc,\n                'id': text_results['ids'][0][i],\n                'page': text_results['metadatas'][0][i]['page'],\n                'score': text_results['distances'][0][i] if 'distances' in text_results else 0\n            })\n        \n        # Format image results\n        retrieved_images = []\n        for i, img_id in enumerate(image_results['ids'][0]):\n            image_num = image_results['metadatas'][0][i]['image_num']\n            retrieved_images.append({\n                'id': img_id,\n                'image_num': image_num,\n                'page': image_results['metadatas'][0][i]['page'],\n                'type': image_results['metadatas'][0][i]['type'],\n                'caption': image_results['metadatas'][0][i]['caption'],\n                'score': image_results['distances'][0][i] if 'distances' in image_results else 0\n            })\n            \n        logger.info(f\"Retrieved {len(retrieved_texts)} text chunks and {len(retrieved_images)} images\")\n        return retrieved_texts, retrieved_images\n\n    def rerank_results(self, query: str, retrieved_texts: List[Dict], retrieved_images: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Rerank retrieval results for better relevance\n        \n        Args:\n            query: The query string\n            retrieved_texts: List of retrieved text chunks\n            retrieved_images: List of retrieved images\n            \n        Returns:\n            tuple: (reranked_texts, reranked_images)\n        \"\"\"\n        \n        # Get query keywords for keyword matching\n        query_keywords = set(query.lower().split())\n        \n        # Rerank texts based on keyword matches and semantic similarity\n        for text in retrieved_texts:\n            content = text['content'].lower()\n            # Count keyword matches\n            keyword_matches = sum(1 for keyword in query_keywords if keyword in content)\n            # Adjust score with keyword bonus\n            text['score'] = text['score'] + (keyword_matches * 0.05)\n            \n        # Sort by adjusted score (lower is better in this case)\n        reranked_texts = sorted(retrieved_texts, key=lambda x: x['score'])\n        \n        # For images, we'll rely on the initial ranking from the vector DB\n        reranked_images = retrieved_images\n        \n        return reranked_texts, reranked_images\n\n    def generate_response(self, query: str, retrieved_texts: List[Dict], retrieved_images: List[Dict]) -> Tuple[str, int]:\n        \"\"\"\n        Generate a response based on retrieved information\n        \n        Args:\n            query: The query string\n            retrieved_texts: List of retrieved text chunks\n            retrieved_images: List of retrieved images\n            \n        Returns:\n            tuple: (response_text, image_number)\n        \"\"\"\n        logger.info(\"Generating response...\")\n        \n        # Format context from retrieved texts\n        context_parts = []\n        for i, text in enumerate(retrieved_texts):\n            context_parts.append(f\"Text {i+1} (Page {text['page']+1}):\\n{text['content']}\")\n        \n        context = \"\\n\\n\".join(context_parts)\n        \n        # Format image information\n        image_info = []\n        for img in retrieved_images:\n            image_info.append(f\"{img['type']} {img['image_num']} (Page {img['page']+1}): {img['caption']}\")\n        \n        image_context = \"\\n\".join(image_info) if image_info else \"No relevant figures or tables found.\"\n        \n        # For LLM generation, we'll use an API call\n        prompt = f\"\"\"\n        You are an expert assistant helping with economic document analysis.\n        \n        User Query: {query}\n        \n        Below are the most relevant text excerpts from the document:\n        \n        {context}\n        \n        Relevant Figures/Tables:\n        {image_context}\n        \n        Please provide a comprehensive and accurate answer to the query based ONLY on the information provided above.\n        \n        If specific figures or tables are relevant to your answer, please reference them by their number.\n        Do not make up information that isn't in the provided context.\n        Write in a professional, academic style appropriate for economic analysis.\n        \"\"\"\n        \n        try:\n            if self.config.get(\"use_openai\", False):\n                response_text = self._call_openai_api(prompt)\n            else:\n                # Simulate a response based on retrieved info\n                response_text = self._simulate_response(query, retrieved_texts, retrieved_images)\n                \n            # Determine most relevant image number (if any)\n            image_num = 0\n            if retrieved_images:\n                # Choose the highest ranked image\n                image_num = retrieved_images[0]['image_num']\n                \n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\")\n            response_text = \"Error generating response.\"\n            image_num = 0\n            \n        return response_text, image_num\n    \n    def _call_openai_api(self, prompt):\n        \"\"\"Call OpenAI API for response generation\"\"\"\n        # Replace with your actual API call\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.config['openai_api_key']}\"\n        }\n        \n        data = {\n            \"model\": \"gpt-4-turbo\",\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in economic analysis.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"temperature\": 0.3,\n            \"max_tokens\": 800\n        }\n        \n        response = requests.post(\n            \"https://api.openai.com/v1/chat/completions\",\n            headers=headers,\n            json=data\n        )\n        \n        if response.status_code == 200:\n            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n        else:\n            raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n    \n    def _simulate_response(self, query, retrieved_texts, retrieved_images):\n        \"\"\"Simulate an LLM response (fallback when API isn't available)\"\"\"\n        # Simple simulation of response generation by combining parts of retrieved texts\n        response_parts = []\n        \n         # Add introduction\n        response_parts.append(f\"Based on the economic document, here's the answer to your query about '{query}':\")\n       \n        # Add content from retrieved texts\n        used_content = set()\n        for text in retrieved_texts[:3]:  # Use top 3 texts\n            sentences = text['content'].split('.')\n            for sentence in sentences[:3]:  # Use up to 3 sentences from each text\n                clean_sentence = sentence.strip()\n                if clean_sentence and clean_sentence not in used_content and len(clean_sentence) > 30:\n                    response_parts.append(clean_sentence + '.')\n                    used_content.add(clean_sentence)\n        \n        # Add references to images if available\n        if retrieved_images:\n            img = retrieved_images[0]\n            response_parts.append(f\"As shown in {img['type']} {img['image_num']}, this economic trend is illustrated graphically.\")\n        \n        # Combine parts\n        response = \" \".join(response_parts)\n        return response\n    \n    def evaluate_bertscore(self, generated_responses, reference_responses):\n        \"\"\"\n        Evaluate responses using BERTScore\n        \n        Args:\n            generated_responses: List of generated responses\n            reference_responses: List of reference responses\n            \n        Returns:\n            float: BERTScore F1 score\n        \"\"\"\n        P, R, F1 = score(generated_responses, reference_responses, lang=\"en\", verbose=True)\n        bert_score = F1.mean().item()\n        logger.info(f\"BERTScore: {bert_score}\")\n        return bert_score\n    \n    def process_questions(self, questions_csv: str) -> pd.DataFrame:\n        \"\"\"\n        Process all questions and generate answers\n        \n        Args:\n            questions_csv: Path to CSV file with questions\n            \n        Returns:\n            DataFrame: Results with answers\n        \"\"\"\n        logger.info(f\"Processing questions from {questions_csv}\")\n        questions_df = pd.read_csv(questions_csv)\n        \n        results = []\n        for _, row in tqdm(questions_df.iterrows(), total=len(questions_df)):\n            query = row['Question']\n            query_id = row['ID']\n            \n            # Retrieve information\n            retrieved_texts, retrieved_images = self.retrieve(query)\n            \n            # Rerank results\n            reranked_texts, reranked_images = self.rerank_results(query, retrieved_texts, retrieved_images)\n            \n            # Generate response\n            response_text, image_num = self.generate_response(query, reranked_texts, reranked_images)\n            \n            results.append({\n                'ID': query_id,\n                'Text': response_text,\n                'Image': image_num\n            })\n            \n            # Clean up to prevent memory leaks\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        \n        results_df = pd.DataFrame(results)\n        return results_df\n    \n    def save_results(self, results_df: pd.DataFrame, output_csv: str):\n        \"\"\"\n        Save results to CSV file\n        \n        Args:\n            results_df: DataFrame with results\n            output_csv: Path to output CSV file\n        \"\"\"\n        results_df.to_csv(output_csv, index=False)\n        logger.info(f\"Results saved to {output_csv}\")\n    \n    def run_pipeline(self, pdf_path: str, questions_csv: str, output_csv: str):\n        \"\"\"\n        Run the complete RAG pipeline\n        \n        Args:\n            pdf_path: Path to PDF document\n            questions_csv: Path to questions CSV\n            output_csv: Path to save results\n        \"\"\"\n        # Process PDF and extract content\n        text_chunks, images = self.process_pdf(pdf_path)\n        \n        # Generate embeddings\n        text_embeddings, image_embeddings = self.generate_embeddings(text_chunks, images)\n        \n        # Store in vector DB\n        self.store_in_db(text_embeddings, image_embeddings)\n        \n        # Process questions and generate answers\n        results_df = self.process_questions(questions_csv)\n        \n        # Save results\n        self.save_results(results_df, output_csv)\n        \n        return results_df\n\n# Configuration\nconfig = {\n    \"text_embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n    \"clip_model\": \"openai/clip-vit-base-patch32\",\n    \"chunk_size\": 500,\n    \"chunk_overlap\": 100,\n    \"chroma_db_path\": \"./chroma_db\",\n    \"use_openai\": True,  \n    \"openai_api_key\": \"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\"  # Replace with your API key if use_openai is True\n}\n\n\ndef main():\n    # Initialize the MultiModal RAG system\n    rag_system = MultiModalRAG(config)\n    \n    # Run the pipeline\n    results = rag_system.run_pipeline(\n        pdf_path= \"/kaggle/input/data-266-multi-modal-rag/document.pdf\" ,\n        questions_csv=\"/kaggle/input/questions/Lab_2_Part_1_Questions.csv\",\n        output_csv=\"answers.csv\"\n    )\n    \n    print(\"Pipeline completed successfully!\")\n    print(f\"Generated answers for {len(results)} questions\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T04:33:37.264046Z","iopub.execute_input":"2025-05-03T04:33:37.264465Z","iopub.status.idle":"2025-05-03T04:35:46.276961Z","shell.execute_reply.started":"2025-05-03T04:33:37.264430Z","shell.execute_reply":"2025-05-03T04:35:46.276139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2487a55e2b64a28b3474f44215af6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d619a828c924da09118030cc8024156"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4054dacaca4bc1ab2de298f552752a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee354125e0b4c7c91c045e26f7190c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84cc451dc9146639e8354d070ea2807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"723c9fa1532a43a88cf9a8adc983780f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117d983845e84a30bbe3c7e10a1a4916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1214e2d3c12f4406bb3919cedd6b3c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95feac84e84b476ea39387ddb9bdfccb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ebf004c14a54defaed41f30c6fcf185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9df2ea430e4446a95f6328e87ec48de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25bda808f8c4ceba6743a8fd2342f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3569248c9fc4439993e4ad888a9dbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06b720b94a9466fb99a5599e286915b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f82fd16c56645bb9f693a172a3ade88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d2c4be9d0a40f59399244a7f13f041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb01e813bce84b238c85695a2a2824cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c147ded6354ead9a74cf0f53f6bd94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab54bc917db4e519766b50e83cdc6c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"612dbd00a34d4c2aace4e9c46ce2469b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692fe95dd2bc4827ba07d61224c70f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366de072f4024ea9a8f6ce6331888be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a8796abe994a349d36265acea06707"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e126b035f7b44768995adefe638b8945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea82e9db9e4489db2cf5d81db2996a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f5b042df57411cbf488b46106a309a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cfe393191d5419ebdfe9241ad1cd107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28de98c271944df9a3bd59431f97f773"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8c8370038f4a47a45466cadb81ef0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b3acd1f8de4897989e585d23be831a"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/11 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0703b4a4a84ab49e026800aa382083"}},"metadata":{}},{"name":"stderr","text":"  9%|▉         | 1/11 [00:12<02:05, 12.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932acbf2dfaf4232b997be6922c69bfe"}},"metadata":{}},{"name":"stderr","text":" 18%|█▊        | 2/11 [00:22<01:36, 10.74s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc78ef20dcff4d0a8327996a38f607d0"}},"metadata":{}},{"name":"stderr","text":" 27%|██▋       | 3/11 [00:35<01:35, 11.96s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448cd3add4bc4b3c94e79de618c4b851"}},"metadata":{}},{"name":"stderr","text":" 36%|███▋      | 4/11 [00:45<01:19, 11.35s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b535e850c0634090a29cb124da170b28"}},"metadata":{}},{"name":"stderr","text":" 45%|████▌     | 5/11 [00:53<01:00, 10.07s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fc834466f54435958f586c8819b850"}},"metadata":{}},{"name":"stderr","text":" 55%|█████▍    | 6/11 [01:00<00:45,  9.03s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807312776fa94d8fadc7f37a44d4e383"}},"metadata":{}},{"name":"stderr","text":" 64%|██████▎   | 7/11 [01:07<00:33,  8.39s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10958c1a52c4b7ca96f68085c2965a5"}},"metadata":{}},{"name":"stderr","text":" 73%|███████▎  | 8/11 [01:19<00:28,  9.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4428399bd89b4673b4495dc2fb748151"}},"metadata":{}},{"name":"stderr","text":" 82%|████████▏ | 9/11 [01:29<00:19,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be7d8799dd04213809e4bdc1384853b"}},"metadata":{}},{"name":"stderr","text":" 91%|█████████ | 10/11 [01:40<00:10, 10.13s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f4bbce82444165a72288c3d1f75d77"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 11/11 [01:50<00:00, 10.03s/it]","output_type":"stream"},{"name":"stdout","text":"Pipeline completed successfully!\nGenerated answers for 11 questions\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}